{"cells":[{"cell_type":"code","metadata":{"id":"a3PnsyICVY2v","colab_type":"code","colab":{}},"source":["## いじるのはこのセルのパラメータのみでOK\n","\n","file_name_download = \"IPB-20210622docx\" #ダウンロードされる業績リストのファイル名\n","file_name_download_xlsx = \"IPB-202106022xlsx\" #ダウンロードされるxlsxファイルのファイル名\n","file_name_download_check = \"IPB-202106022check.docx\" #集計チェック用のファイル名\n","\n","globalmindate='2019-06-28' #これより後の業績を集める\n","globalmaxdate='2021-07-01' #これより前の業績を集める\n","smark='◎' #researchmapで課題番号紐づけありの論文にマーク付ける場合はここで指定。\n","ryoiki_linked = False #researchmapで課題番号紐づけありの論文のみ出力したい場合はTrue\n","allenglish = False #名前表記をすべて英語で統一する場合はTrue, 論文以外の名前表記を日本語にする場合False\n","SNfirst = False #英語名前表記をすべて名字先で統一する場合はTrue, 名字後で統一する場合はFalse\n","numberingPapers = True #出力の際に論文をナンバリング\n","peer_reviewed = True #査読ありのチェックが入った論文だけに限定する場合はTrue\n","firstnameInitial = True\n","\n","sankodata=True # xlsxファイルの読み書きをする場合\n","\n","docoutputpointsize=11 #11pt出力指定\n","\n","# 領域メンバーの情報入りスプレッドシートのURL\n","sheeturl='https://docs.google.com/spreadsheets/d/1wce1XHSFGSBttupnSIqe_5abtijBb_hBYM2bfaV9Jn4/edit#gid=0' \n","\n","# 未入力のxlsxファイルのURL (githubにアップロード済)\n","blankxlsx='https://github.com/dbkk/docx-researchmap/blob/rev2021/inputfiles/R3%E4%B8%AD%E9%96%93%E8%A9%95%E4%BE%A1%E5%A0%B1%E5%91%8A%E6%9B%B8%EF%BC%881_%E9%A0%98%E5%9F%9F%E5%85%A8%E4%BD%93%EF%BC%89%EF%BC%8813%E5%8F%82%E8%80%83%E3%83%87%E3%83%BC%E3%82%BFExcel%E7%89%88%EF%BC%89.xlsx?raw=true'\n","\n","# google formで取ったそれ以外のデータ (非公開) (readme.md参照)\n","sheeturlform=''\n","\n","#出力時の上限数\n","maxpap=99#9\n","maxtalk=99#5\n","maxsocial=99#5\n","maxmed=99#5\n","maxsonota=99#3\n","maxBSM=99#10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rIZIv9QVY3G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"d2e32d75-856a-4695-b1b6-858a7b64442e","executionInfo":{"status":"ok","timestamp":1584094456008,"user_tz":-540,"elapsed":3508,"user":{"displayName":"深井洋佑","photoUrl":"","userId":"07275761346057679589"}}},"source":["import requests,json,sys,os,gspread,time,re,openpyxl,datetime,xlrd\n","import numpy as np\n","import pandas as pd\n","\n","if 'google.colab' in str(get_ipython()):\n","    %pip install python-docx\n","    from google.colab import files,auth\n","    from oauth2client.client import GoogleCredentials\n","    outputdirectory = ''\n","else:\n","    outputdirectory = '../docx-researchmap-outputs/' #ローカルで実行する場合は保存ファイルのディレクトリを適当に指定\n","    os.makedirs(outputdirectory,exist_ok=True)\n","from docx import Document\n","from docx.shared import Pt,Mm,RGBColor\n","from docx.enum.text import WD_UNDERLINE,WD_LINE_SPACING,WD_BREAK\n","\n","file_name=outputdirectory+file_name_download\n","file_name_xlsx=outputdirectory+file_name_download_xlsx\n","file_name_check=outputdirectory+file_name_download_check"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlnwDJeCW4PW","colab_type":"code","colab":{}},"source":["#スプレッドシートをダウンロード\n","sheeturl_csv=re.match(\"https://docs.google.com/spreadsheets/d/.+/\",sheeturl).group(0)+\"export?format=csv\"\n","name_data=pd.read_csv(sheeturl_csv)\n","name_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1E8b1HjY3oN-","colab":{}},"source":["membernum=len(name_data)\n","\n","if SNfirst:\n","    allnames=(name_data[\"Surname\"]+' '+name_data[\"First name\"]).to_list()\n","else:\n","    allnames=(name_data[\"First name\"]+' '+name_data[\"Surname\"]).to_list()\n","allSurname=name_data[\"Surname\"].to_list()\n","allnamesJP=(name_data[\"苗字\"]+\" \"+name_data[\"名\"]).to_list()\n","allgroupnames=name_data[\"班\"].to_list()\n","allgroupnum=name_data[\"番号\"].to_list()\n","allmembers=name_data[\"researchmapID\"].to_list()\n","allDB=name_data[\"代表分担協力\"].values\n","allkeikaku=[b for a,b in zip(allgroupnames,allnamesJP) if a in ['A','B','C']]\n","allkeikakuPIs=[b for a,b,c in zip(allgroupnames,allnamesJP,allDB) if (a in ['A','B','C']) & (c =='D')]\n","allDaihyoBuntan=list(allDB)\n","allHan=(name_data[\"班\"]+name_data[\"番号\"].apply(str)).to_list()\n","grant_numbers=name_data[\"grantID\"].to_list()\n","allmindate=name_data[\"Start date\"].to_list()\n","allmaxdate=name_data[\"End date\"].to_list()\n","\n","#Exception names handling\n","altname2,altname3=name_data['著者名（2個目）'],name_data['著者名（3個目）']\n","arraltname2,arraltname3=altname2.values,altname3.values\n","nameList=allnames+list(arraltname2[~(pd.isna(altname2).values)])+list(arraltname3[~(pd.isna(altname3).values)])\n","nameList = [n.strip() for n in nameList]\n","daihyobuntanList=allDaihyoBuntan+list(allDB[~(pd.isna(altname2).values)])+list(allDB[~(pd.isna(altname3).values)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FNRoF4CLh0lq","colab":{}},"source":["# Function to set the name order (surname-firstname)\n","def SurnameFirst(namesDic,sn):\n","    oldnamelist=[]\n","    swap=0\n","    for indiv in namesDic:\n","        oldnamelist=oldnamelist+[indiv['name'].replace(',','').replace('.','')]\n","        #print(oldnamelist)\n","    for name in oldnamelist:\n","        if sn in name.split(' '):\n","            if name.split(' ').index(sn)==0: # surname first\n","                swap= True ^ SNfirst\n","                break;\n","            else:\n","                swap= False ^ SNfirst\n","                break;\n","    swapcheck=swap\n","    if swap:\n","        newnamelist=[]\n","        for name in oldnamelist:\n","            namesplit=name.split(' ')\n","            names=[namesplit[-1]]+namesplit[:-1]\n","            newnamelist=newnamelist+[' '.join(names)]\n","    else:\n","        newnamelist=oldnamelist\n","    \n","    if SNfirst & firstnameInitial:\n","        holdlist=[]\n","        for name in newnamelist:\n","            namesplit=name.split(' ')\n","            names=[namesplit[0]]+[', ']+[namesplit[:-1][0]]+['.']\n","            holdlist=holdlist+[''.join(names)]\n","        newnamelist=holdlist\n","    elif firstnameInitial:\n","        holdlist=[]\n","        for name in newnamelist:\n","            namesplit=name.split(' ')\n","            sn=namesplit[-1]\n","            sn=sn.lower()\n","            sn=sn[0].upper()+sn[1:]\n","            names=[namesplit[0][0]]+['. ']+[sn]\n","            holdlist=holdlist+[''.join(names)]\n","        newnamelist=holdlist                    \n","\n","\n","    return newnamelist\n","\n","def ReturnDictWOerror(dictdata,key,nodata):\n","    if key in dictdata.keys():\n","        return dictdata[key]\n","    else:\n","        return nodata\n","\n","def ReturnDictContent(dictdata,key,key1,nodata=''):\n","    d=ReturnDictWOerror(dictdata,key,nodata)\n","    d1=ReturnDictWOerror(dictdata,key1,nodata)\n","    if d!=nodata:\n","        return d\n","    else:\n","        return d1\n","\n","def commaR(vol,spage):\n","    if (vol=='') & (spage==''):\n","        return ''\n","    elif (vol=='') | (spage==''):\n","        return ' '\n","    else:\n","        return ', '"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["url = \"https://api.researchmap.jp/\"\n","itemslist = [\"published_papers\",\"research_projects\",\"misc\",\"presentations\",\"books_etc\",\"social_contribution\",\"awards\",\"media_coverage\"]\n","jsonfiles={}\n","for name in allmembers:\n","    print('downloading: '+name)\n","    jsonfiles[name]={}\n","    for it in itemslist:\n","        r1 = requests.get(url+name+'/'+it)\n","        jsonfiles[name][it]=json.loads(r1.text)\n","        if 'error' in jsonfiles[name][it].keys():\n","            print(jsonfiles[name][it]['error'])\n","            print(\"  error in:\"+it)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"96xz8YVTh0ly","colab":{}},"source":["# make dictionary of all papers\n","i=0\n","PapersDict={}\n","\n","doilist=[]\n","doiDict={}\n","titlelist=[]\n","titleDict={}\n","#dc = Document()\n","for ids,fullname,dh,mindate,maxdate,han in zip(allmembers,allnames,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","    surname=fullname.split(' ')[0 if SNfirst else 1]\n","    dfP = jsonfiles[ids][\"published_papers\"]\n","    dfG = jsonfiles[ids][\"research_projects\"]\n","    if 'items' in dfG.keys():\n","        grantID=\"0\"\n","        for dfs in dfG['items']:\n","            if 'identifiers' in dfs.keys():\n","                if 'grant_number' in dfs['identifiers'].keys():\n","                    if dfs['identifiers']['grant_number'][0] in grant_numbers:\n","                        grantID=dfs['rm:id']\n","                        break\n","    if 'items' in dfP.keys():    \n","        for dfs in dfP['items']:\n","            if \"authors\" not in dfs.keys():\n","                continue\n","            if ('identifiers' in dfs.keys()) & (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","                doinum=[0]\n","                if 'doi' in dfs['identifiers'].keys():\n","                    doinum=dfs['identifiers']['doi']\n","\n","                PapersDict[i]={}\n","                PapersDict[i]['issues']=False\n","                PapersDict[i]['preprint']=False\n","                correspo=False\n","                Ryoiki=False\n","                if 'rm:research_project_id' in dfs['identifiers'].keys():\n","                    if grantID in dfs['identifiers']['rm:research_project_id']:\n","                        Ryoiki=True\n","                        \n","                if \"published_paper_owner_roles\" in dfs.keys():\n","                    if (\"corresponding\" in dfs[\"published_paper_owner_roles\"]) | (\"last\" in dfs[\"published_paper_owner_roles\"]):\n","                        correspo=True\n","\n","                jname=''        \n","                if \"publication_name\" in dfs.keys():\n","                    jname=ReturnDictContent(dfs[\"publication_name\"],'en','ja','').upper()\n","\n","                if jname =='ARXIV':\n","                    PapersDict[i]['preprint']=True\n","                    if \"arxiv_id\" in dfs['identifiers'].keys():\n","                        jname=dfs['identifiers']['arxiv_id'][0] + ' (preprint)'\n","                    else:\n","                        jname='arxiv'\n","                \n","                if not(\"publication_name\" in dfs.keys()):\n","                    if \"arxiv_id\" in dfs['identifiers'].keys():\n","                        jname=dfs['identifiers']['arxiv_id'][0] + ' (preprint)'\n","                        PapersDict[i]['preprint']=True\n","                    elif doinum[0]!=0:\n","                        jname='DOI: '+doinum[0]\n","                        PapersDict[i]['preprint']=True\n","                    else:\n","                        jname='journal unspecified'\n","                        PapersDict[i]['issues']=True\n","                    \n","                Sname=SurnameFirst(ReturnDictContent(dfs[\"authors\"],'en','ja',''),surname)\n","\n","                spage=''\n","                if \"starting_page\" in dfs.keys():\n","                    if dfs[\"starting_page\"]!='':\n","                        spage=dfs[\"starting_page\"]\n","\n","                vol=''\n","                if \"volume\" in dfs.keys():\n","                    if dfs[\"volume\"]!='':\n","                        vol=' '+dfs[\"volume\"]\n","                if doinum in doilist:\n","                    doiDict[doinum[0]]['name']=doiDict[doinum[0]]['name']+[fullname]\n","                    doiDict[doinum[0]]['Corresp']=doiDict[doinum[0]]['Corresp']+[correspo]\n","                else:\n","                    doiDict[doinum[0]]={}\n","                    doiDict[doinum[0]]['name']=[fullname]\n","                    doiDict[doinum[0]]['Corresp']=[correspo]\n","                    doiDict[doinum[0]]['count']=0\n","                    doilist=doilist+[doinum[0]]\n","                \n","                papertitle=ReturnDictContent(dfs['paper_title'],'en','ja','')\n","                papid=papertitle.upper().rstrip('.')\n","\n","                if papid in titlelist:\n","                    titleDict[papid]['name'] = titleDict[papid]['name']+[fullname]\n","                    titleDict[papid]['Corresp'] = titleDict[papid]['Corresp']+[correspo]                    \n","                else:\n","                    titlelist = titlelist + [papid]\n","                    titleDict[papid] = {}\n","                    titleDict[papid]['name'] = [fullname]\n","                    titleDict[papid]['Corresp'] = [correspo]\n","                    titleDict[papid]['count']=0\n","\n","                text1=\"\\\"\"+papertitle+\"\\\"\" +', '\n","                text2=jname+','+vol+commaR(vol,spage)+spage+ ' ('+dfs[\"publication_date\"][:4] +').'\n","                if \"description\" in dfs.keys():\n","                    PapersDict[i]['oudan']=ReturnDictWOerror(dfs[\"description\"],'ja','')\n","                else:\n","                    PapersDict[i]['oudan']=''\n","                #print(PapersDict[i]['oudan'])\n","                PapersDict[i]['kokunai']=False\n","                if \"is_international_journal\" in dfs.keys():\n","                    if not dfs[\"is_international_journal\"]:\n","                        PapersDict[i]['kokunai']=True\n","                PapersDict[i]['text1']=text1\n","                PapersDict[i]['text2']=text2\n","                PapersDict[i]['papid']=papid\n","                PapersDict[i]['researcher']=fullname\n","                PapersDict[i]['authors']=Sname\n","                PapersDict[i]['date']=dfs[\"publication_date\"]\n","                PapersDict[i]['referee']=ReturnDictContent(dfs,'referee','referee',False)\n","                PapersDict[i]['doi']=doinum[0]\n","                PapersDict[i]['ryoiki']=Ryoiki\n","                PapersDict[i]['Daihyo']=dh\n","                PapersDict[i]['han']=han\n","                PapersDict[i]['Corresp']=correspo\n","                i=i+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cw-jxR49h0l1","colab":{},"tags":[]},"source":["# make dictionary of all talks, 'social_contribution', awards\n","TalksDict={}\n","SocialContDict={}\n","AwardsDict={}\n","i,j,l=0,0,0\n","for ids,fullname,fullnameJP,dh,mindate,maxdate,han in zip(allmembers,allnames,allnamesJP,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","    dfPr = jsonfiles[ids][\"presentations\"]\n","    dfSC = jsonfiles[ids][\"social_contribution\"]\n","    dfAw = jsonfiles[ids][\"awards\"]\n","    if 'items' in dfPr.keys():\n","        for dfs in dfPr['items']:\n","            if all([a in dfs.keys() for a in [\"presentation_title\",\"event\",'publication_date','presenters']]):\n","                if (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","                    if ('en' in dfs[\"presenters\"].keys()):\n","                        pname=dfs[\"presenters\"][\"en\"][0][\"name\"]\n","                    else:\n","                        pname=dfs[\"presenters\"][\"ja\"][0][\"name\"]\n","                    ename=ReturnDictContent(dfs[\"event\"],'ja','en','')\n","                    ptitle=ReturnDictContent(dfs[\"presentation_title\"],'ja','en','')\n","                    pdate=dfs[\"publication_date\"]\n","                    TalksDict[i]={}\n","                    TalksDict[i][\"presenter\"]=fullnameJP\n","                    if allenglish:\n","                        TalksDict[i]['printname']=fullname\n","                    else:\n","                        TalksDict[i][\"printname\"]=fullnameJP\n","                    TalksDict[i][\"event\"]=ename\n","                    TalksDict[i][\"presentation_title\"]=ptitle\n","                    TalksDict[i][\"date\"]=pdate\n","                    TalksDict[i][\"han\"]=han\n","                    TalksDict[i][\"invited\"]=ReturnDictWOerror(dfs,'invited',False)\n","                    TalksDict[i][\"international\"]=ReturnDictWOerror(dfs,'is_international_presentation',False)\n","                    TalksDict[i][\"keyoral\"]= ReturnDictWOerror(dfs,\"presentation_type\",'')\n","                    i=i+1\n","    if 'items' in dfSC.keys():\n","        for dfs in dfSC['items']:\n","            if 'from_event_date' in dfs.keys():\n","                if (dfs['from_event_date']>=mindate) & (dfs['from_event_date']<=maxdate):\n","                    SocialContDict[j]={}\n","                    SocialContDict[j]['name']=fullnameJP\n","                    SocialContDict[j][\"title\"]=dfs['social_contribution_title']['ja']\n","                    SocialContDict[j][\"date\"]=dfs['from_event_date']\n","                    SocialContDict[j][\"han\"]=han\n","                    if 'event' in dfs.keys():\n","                        SocialContDict[j][\"event\"]=ReturnDictContent(dfs[\"event\"],'ja','en','')\n","                    else:\n","                        SocialContDict[j][\"event\"]=''\n","                    j=j+1\n","    if 'items' in dfAw.keys():\n","        for dfs in dfAw['items']:\n","            if (dfs['award_date']>=mindate) & (dfs['award_date']<=maxdate):\n","                AwardsDict[l]={}\n","                AwardsDict[l]['name']=fullnameJP\n","                AwardsDict[l]['award_name']=ReturnDictContent(dfs['award_name'],'ja','en','')\n","                if 'association' in dfs.keys():\n","                    AwardsDict[l]['association']=ReturnDictContent(dfs['association'],'ja','en','')\n","                else:\n","                    AwardsDict[l]['association']=''\n","                AwardsDict[l]['award_date']=dfs['award_date']\n","                l=l+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hctp5YUnh0l4","colab":{}},"source":["# make dictionary of all books_etc\n","booksDict={}\n","i=0\n","for ids,fullname,fullnameJP,dh,mindate,maxdate,han in zip(allmembers,allnames,allnamesJP,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","  dfM = jsonfiles[ids][\"books_etc\"]\n","  if 'items' in dfM.keys():\n","    for dfs in dfM['items']:\n","      if all([a in dfs.keys() for a in ['authors',\"book_title\",\"publication_date\"]]):\n","        if (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","          if ('ja' in dfs[\"authors\"].keys()):\n","              pname=dfs[\"authors\"][\"ja\"][0][\"name\"]\n","          else:\n","              pname=dfs[\"authors\"][\"en\"][0][\"name\"]\n","          ename=ReturnDictContent(dfs[\"book_title\"],'ja','en','')\n","          if \"book_owner_range\" in dfs.keys():\n","            eoname=\" \\'\"+ReturnDictContent(dfs[\"book_owner_range\"],'ja','en','')+\"\\',\"\n","          else:\n","            eoname=''\n","          if \"book_owner_role\" in dfs.keys():\n","            brole=\" (\"+dfs[\"book_owner_role\"]+\"),\"\n","          else:\n","            brole=','\n","          if \"publisher\" in dfs.keys():\n","            pub=\" \"+ReturnDictContent(dfs[\"publisher\"],'ja','en','')+\",\"\n","          else:\n","            pub=''\n","          pdate=dfs[\"publication_date\"]\n","          booksDict[i]={}\n","          booksDict[i]['authors']=fullname\n","          if allenglish:\n","            booksDict[i]['printname']=fullname\n","          else:\n","            booksDict[i]['printname']=fullnameJP\n","          booksDict[i][\"book_title\"]=' '+ename+','\n","          booksDict[i][\"book_owner_role\"]=brole\n","          booksDict[i][\"book_owner_range\"]=eoname\n","          booksDict[i][\"publisher\"]=pub\n","          booksDict[i][\"date\"]=pdate\n","          booksDict[i][\"han\"]=han\n","          i=i+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nFe7cnwg6qNv","colab":{}},"source":["# make dictionary of all MISCs\n","miscDict={}\n","medDict={}\n","i,j=0,0\n","for ids,fullname,fullnameJP,dh,mindate,maxdate,han in zip(allmembers,allnames,allnamesJP,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","  dfMis = jsonfiles[ids][\"misc\"]\n","  if 'items' in dfMis.keys():\n","    for dfs in dfMis['items']:\n","      if all([a in dfs.keys() for a in ['authors',\"paper_title\",\"publication_date\",\"publication_name\"]]):\n","        if  (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","          if ('ja' in dfs[\"authors\"].keys()):\n","              pname=dfs[\"authors\"][\"ja\"][0][\"name\"]\n","          else:\n","              pname=dfs[\"authors\"][\"en\"][0][\"name\"]\n","          ename=ReturnDictContent(dfs[\"paper_title\"],'ja','en','')\n","          ptitle=ReturnDictContent(dfs[\"publication_name\"],'ja','en','')\n","          pdate=dfs[\"publication_date\"]\n","          miscDict[i]={}\n","          miscDict[i]['authors']=fullname\n","          if allenglish:\n","            miscDict[i]['printname']=fullname\n","          else:\n","            miscDict[i]['printname']=fullnameJP\n","\n","          miscDict[i][\"paper_title\"]=' \\''+ename+'\\','\n","          miscDict[i][\"publication_name\"]=' '+ptitle+','\n","          miscDict[i][\"date\"]=pdate\n","          miscDict[i][\"han\"]=han\n","          i=i+1\n","  dfMed = jsonfiles[ids][\"media_coverage\"]\n","  if 'items' in dfMed.keys():\n","    for dfs in dfMed['items']:\n","      if all([a in dfs.keys() for a in [\"media_coverage_title\",\"publication_date\"]]):\n","        if  (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","          ename=ReturnDictContent(dfs[\"media_coverage_title\"],'ja','en','')\n","          \n","          \n","          pdate=dfs[\"publication_date\"]\n","          medDict[j]={}\n","          medDict[j]['authors']=fullname\n","          if allenglish:\n","            medDict[j]['printname']=fullname\n","          else:\n","            medDict[j]['printname']=fullnameJP\n","          medDict[j][\"media_coverage_type\"]=''\n","          if \"media_coverage_type\" in dfs.keys():\n","            medDict[j][\"media_coverage_type\"]=dfs[\"media_coverage_type\"]\n","          ptitle=''\n","          if \"publisher\" in dfs.keys():\n","            ptitle=ReturnDictContent(dfs[\"publisher\"],'ja','en','')+', '\n","\n","          petitle=''\n","          if \"event\" in dfs.keys():\n","            petitle=ReturnDictContent(dfs[\"event\"],'ja','en','')+','\n","#          print(' '+ptitle+petitle)\n","\n","          pltitle=''\n","          if \"location\" in dfs.keys():\n","            pltitle=ReturnDictContent(dfs[\"location\"],'ja','en','')+','\n","#          print(' '+ptitle+petitle+pltitle)\n","\n","          medDict[j][\"paper_title\"]=' \\''+ename+'\\','\n","          medDict[j][\"publication_name\"]=' '+ptitle+petitle+pltitle\n","          medDict[j][\"date\"]=pdate\n","          medDict[j][\"han\"]=han\n","          j=j+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# generate docx\n","document = Document()\n","\n","section = document.sections[0]\n","section.left_margin = Mm(15)\n","section.right_margin = Mm(15)\n","section.top_margin = Mm(15)\n","section.bottom_margin = Mm(15)\n","\n","if peer_reviewed:\n","    refbool=[True]\n","else:\n","    refbool=[True,False]\n","\n","if ryoiki_linked:\n","    ryoikibool=[True]\n","else:\n","    ryoikibool=[True,False]\n","\n","for han in np.unique(allHan):\n","\n","    PapersDictSelected={k:PapersDict[k] for k in range(len(PapersDict)) if (PapersDict[k]['date']>globalmindate) & (PapersDict[k]['date']<globalmaxdate)  & (PapersDict[k]['referee'] in refbool) & (PapersDict[k]['han']==han) & (PapersDict[k]['ryoiki'] in ryoikibool)}\n","    TalksDictSelected={k:TalksDict[k] for k in range(len(TalksDict)) if (TalksDict[k]['date']>globalmindate) & (TalksDict[k]['date']<globalmaxdate) & (TalksDict[k]['han']==han)}\n","    booksDictSelected={k:booksDict[k] for k in range(len(booksDict)) if (booksDict[k]['date']>globalmindate) & (booksDict[k]['date']<globalmaxdate) & (booksDict[k]['han']==han)}\n","    SocialContDictSelected= {k:SocialContDict[k] for k in range(len(SocialContDict)) if (SocialContDict[k]['date']>globalmindate) & (SocialContDict[k]['date']<globalmaxdate) & (SocialContDict[k]['han']==han)}\n","    miscDictSelected={k:miscDict[k] for k in range(len(miscDict)) if (miscDict[k]['date']>globalmindate) & (miscDict[k]['date']<globalmaxdate) & (miscDict[k]['han']==han)}\n","    medDictSelected={k:medDict[k] for k in range(len(medDict)) if (medDict[k]['date']>globalmindate) & (medDict[k]['date']<globalmaxdate) & (medDict[k]['han']==han)}\n","\n","    keys=list(PapersDictSelected.keys())\n","    datelist=[PapersDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","\n","    document.add_paragraph(han[0]+'0'+han[1]+'班')\n","\n","    countBSM=len(booksDictSelected)+len(SocialContDictSelected)+len(miscDictSelected)\n","\n","    if countBSM < maxBSM:\n","        maxpaps=maxpap+maxBSM-countBSM\n","    else:\n","        maxpaps=maxpap\n","    #print(countBSM)\n","    textmax= ', うち'+str(min(maxpaps,len(arg)))+'件抜粋'# if inds>maxpaps else ''\n","    CountR=document.add_paragraph('<原著論文> 査読有計'+str(len(arg))+'件'+textmax)\n","    CountR.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        pap=PapersDictSelected[keys[r]]\n","        ## to eliminate duplicates of papers\n","        # based on DOI\n","        if len(doiDict[pap['doi']]['name'])>1:\n","            if doiDict[pap['doi']]['count']==1:\n","                continue;\n","            titleDict[pap['papid']]['count']=1\n","            doiDict[pap['doi']]['count']=1\n","        # based on paper title\n","        if (len(titleDict[pap['papid']]['name'])>1):\n","            if titleDict[pap['papid']]['count']==1:\n","                continue;\n","            titleDict[pap['papid']]['count']=1\n","            doiDict[pap['doi']]['count']=1\n","        inds=inds+1\n","        if inds<=maxpaps:\n","            if pap['issues']:\n","                p = document.add_paragraph('***')\n","            if numberingPapers:\n","                if pap['ryoiki']:\n","                    p = document.add_paragraph(smark+str(inds)+'. ')\n","                else:\n","                    p = document.add_paragraph(str(inds)+'. ')\n","            else:\n","                if pap['ryoiki']:\n","                    p = document.add_paragraph(smark)\n","                else:\n","                    p = document.add_paragraph('')\n","\n","            for nm in pap['authors']:\n","                if nm in nameList:\n","                    listedCorrespo = any([c for c,n in zip(doiDict[pap['doi']]['Corresp'] + titleDict[pap['papid']]['Corresp'] , doiDict[pap['doi']]['name'] + titleDict[pap['papid']]['name']) if n==nm])\n","                    # print(nm,listedCorrespo)\n","                    if pap['Corresp'] | listedCorrespo:\n","                        p.add_run('*')\n","                    if daihyobuntanList[nameList.index(nm)]=='D':\n","                        p.add_run(nm).underline = WD_UNDERLINE.DOUBLE\n","                    elif daihyobuntanList[nameList.index(nm)]=='B':\n","                        p.add_run(nm).underline = True\n","                    else:\n","                        p.add_run(nm)\n","                else:\n","                    p.add_run(nm)\n","                p.add_run(', ')\n","            p.add_run(pap['text1'])\n","            p.add_run(pap['text2'])\n","\n","    if inds != len(arg):\n","        textmax= ', うち'+str(min(maxpaps,inds))+'件抜粋'# if inds>maxpaps else ''\n","        CountR.text='<原著論文> 査読有計'+str(inds)+'件'+textmax\n","        CountR.runs[0].bold=True\n","        #replaced_text = paragraph.text.replace(\"before\",\"after\")\n","\n","    for r in keys:\n","        doiDict[PapersDictSelected[r]['doi']]['count']=0\n","        titleDict[PapersDictSelected[r]['papid']]['count']=0\n","\n","\n","    keys=list(TalksDictSelected.keys())\n","    datelist=[TalksDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    textmax= ', うち'+str(min(maxtalk,len(arg)))+'件抜粋'# if len(arg)>maxtalk else ''\n","    CountG=document.add_paragraph('<学会発表・講演> 計'+str(len(arg))+'件'+textmax)\n","    CountG.runs[0].bold=True\n","\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxtalk:\n","            pap=TalksDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap[\"presenter\"]\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(', \\\"'+pap[\"presentation_title\"]+\"\\\"\")\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(', '+pap[\"date\"]+'.')\n","\n","    keys=list(booksDictSelected.keys())\n","    datelist=[booksDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    CountB=document.add_paragraph('<書籍> 計'+str(len(arg))+'件')\n","    CountB.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        pap=booksDictSelected[keys[r]]\n","        p = document.add_paragraph(str(inds)+'. ')\n","        nm=pap['authors']\n","        p.add_run(pap[\"printname\"]) \n","        p.add_run(pap[\"book_owner_role\"])\n","        p.add_run(pap[\"book_owner_range\"])\n","        p.add_run(pap[\"book_title\"])\n","        p.add_run(pap[\"publisher\"])\n","        p.add_run(' '+pap[\"date\"][:7]+'.')\n","\n","    keys=list(SocialContDictSelected.keys())\n","    datelist=[SocialContDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    textmax= ', うち'+str(min(maxsocial,len(arg)))+'件抜粋'# if len(arg)>maxsocial else ''\n","    CountO=document.add_paragraph('<アウトリーチ> 計'+str(len(arg))+'件'+textmax)\n","    CountO.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxsocial:\n","            pap=SocialContDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(pap[\"name\"])\n","            p.add_run(', '+pap[\"title\"])\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","\n","    keys=list(medDictSelected.keys())\n","    datelist=[medDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    textmax= ', うち'+str(min(maxmed,len(arg)))+'件抜粋'# if len(arg)>maxmed else ''\n","    CountP=document.add_paragraph('<報道> 計'+str(len(arg))+'件'+textmax)\n","    CountP.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxsonota:\n","            pap=medDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap['authors']\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(','+pap[\"paper_title\"])\n","            p.add_run(pap[\"publication_name\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","\n","    keys=list(miscDictSelected.keys())\n","    datelist=[miscDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    textmax= ', うち'+str(min(maxsonota,len(arg)))+'件抜粋'# if len(arg)>maxsonota else ''\n","    CountM=document.add_paragraph('<その他> 計'+str(len(arg))+'件'+textmax)\n","    CountM.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxsonota:\n","            pap=miscDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap['authors']\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(','+pap[\"paper_title\"])\n","            p.add_run(pap[\"publication_name\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","    p = document.add_paragraph()\n","\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","for paragraph in document.paragraphs:\n","    paragraph.style = document.styles['Normal']\n","    paragraph.paragraph_format.space_before = Pt(2)\n","    paragraph.paragraph_format.space_after = Pt(2)\n","    for run in paragraph.runs:\n","        run.font.size = Pt(docoutputpointsize)\n","document.save(file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 以下「13 参考データ」入力対応\n","if sankodata:\n","    sheeturlform_csv=re.match(\"https://docs.google.com/spreadsheets/d/.+/\",sheeturlform).group(0)+\"export?format=csv\"\n","    col_names = ['c{0:02d}'.format(i) for i in range(46)]\n","    name_dataform=pd.read_csv(sheeturlform_csv,header=0,names=col_names)\n","    name_dataform=name_dataform.sort_values('c00')\n","    name_dataform=name_dataform.drop_duplicates(['c02'],keep='last') # 最新のデータだけ使う\n","    keikakudata=name_dataform[name_dataform['c02'].isin(allkeikaku)]\n","    buntandata=name_dataform[~name_dataform['c02'].isin(allkeikaku)]\n","\n","    timestamps=name_dataform.iloc[:,0].to_list()\n","    namelist=name_dataform.iloc[:,2].to_list()\n","\n","    #keikakudata"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if sankodata:\n","    FYstart=['2019-09-01','2020-04-01']\n","    FYend=['2020-03-31','2021-03-31']\n","    yearcol=['C','E']\n","\n","    if 'google.colab' in str(get_ipython()):\n","        import urllib.request\n","        urllib.request.urlretrieve(blankxlsx, 'blankfile.xlsx')\n","        wb = openpyxl.load_workbook(\"blankfile.xlsx\")\n","    else:\n","        wb = openpyxl.load_workbook(\"./inputfiles/R3中間評価報告書（1_領域全体）（13参考データExcel版）.xlsx\")\n","\n","    ws = wb.worksheets[0]\n","\n","    document = Document()\n","    document.add_paragraph('カウントチェック用出力'+str(datetime.datetime.today()))\n","\n","    section = document.sections[0]\n","    section.left_margin = Mm(15)\n","    section.right_margin = Mm(15)\n","    section.top_margin = Mm(15)\n","    section.bottom_margin = Mm(15)\n","\n","    years=2\n","\n","    # 研究代表者人数\n","    ## R1 計画\n","    ws['C8'].value=len(allkeikakuPIs)\n","\n","    ## R1 公募\n","    ws['D8'].value=0\n","\n","    ## R2 計画\n","    ws['E8'].value=len(allkeikakuPIs)\n","\n","    ## R2 公募\n","    ws['F8'].value=allgroupnames.count('D')\n","\n","    # 研究分担者人数\n","    ## R1 計画\n","    ws['C9'].value=np.sum(keikakudata['c03'])\n","    ## R2 計画\n","    ws['E9'].value=np.sum(keikakudata['c09'])\n","\n","    #研究協力者 若手研究者 外国人研究者 ポスドク RA等\n","    keikakulist=[col+'{0:02d}'.format(i) for col in ['C','E'] for i in range(10,15) ]\n","    buntanlist=[col+'{0:02d}'.format(i)  for col in ['D','F'] for i in range(10,15)]\n","    datapositions=['c'+'{0:02d}'.format(i) for i in [4,5,6,7,8,10,11,12,13,14]]\n","    for d,kei,bun in zip(datapositions,keikakulist,buntanlist):\n","        ws[kei].value=keikakudata[d].astype(float).sum()\n","        ws[bun].value=buntandata[d].astype(float).sum()\n","\n","    papercelllist=[col+'{0:02d}'.format(i) for col in yearcol for i in range(21,25) ]\n","    yearstext=['令和元年度', '令和２年度']\n","    reftext=['査読あり','査読無し']\n","    kokunaitext=['国際誌','国内誌']\n","    OudanDict={}\n","\n","    i=0\n","    for y,yt in zip(range(years),yearstext):\n","        for kokunai,kt in zip([False,True],kokunaitext):\n","            for referee,rt in zip([True,False],reftext):\n","                subdict={k:PapersDict[k] for k in range(len(PapersDict)) if (PapersDict[k]['date']>FYstart[y]) & (PapersDict[k]['date']<=FYend[y])  & (PapersDict[k]['referee'] == referee) & (PapersDict[k]['ryoiki'] in ryoikibool) & (PapersDict[k]['kokunai']==kokunai)}\n","                #print('<'+yt+' '+kt+' '+rt+'>')\n","                document.add_paragraph('<'+yt+' '+kt+' '+rt+'>')\n","                \n","                keys=list(subdict.keys())\n","                datelist=[subdict[r]['date'] for r in keys]\n","                arg=np.argsort(datelist)[::-1]\n","                inds=0\n","                for r in arg:\n","                    inds=inds+1\n","                    pap=subdict[keys[r]]\n","                    ## to eliminate duplicates of papers\n","                    # based on DOI\n","                    if len(doiDict[pap['doi']]['name'])>1:\n","                        if doiDict[pap['doi']]['count']==1:\n","                            continue;\n","                        titleDict[pap['papid']]['count']=1\n","                        doiDict[pap['doi']]['count']=1\n","                    # based on paper title\n","                    if (len(titleDict[pap['papid']]['name'])>1):\n","                        if titleDict[pap['papid']]['count']==1:\n","                            continue;\n","                        titleDict[pap['papid']]['count']=1\n","                        doiDict[pap['doi']]['count']=1\n","                    if pap['issues']:\n","                        p = document.add_paragraph('***')\n","                    if pap['ryoiki']:\n","                        p = document.add_paragraph(smark+str(inds)+'. ')\n","                    else:\n","                        p = document.add_paragraph(str(inds)+'. ')\n","                        p.add_run('(紐づけなし) ').font.color.rgb= RGBColor(255,0,0)\n","                    for nm in pap['authors']:\n","                        if nm in nameList:\n","                            listedCorrespo = any([c for c,n in zip(doiDict[pap['doi']]['Corresp'] + titleDict[pap['papid']]['Corresp'] , doiDict[pap['doi']]['name'] + titleDict[pap['papid']]['name']) if n==nm])\n","                            # print(nm,listedCorrespo)\n","                            if pap['Corresp'] | listedCorrespo:\n","                                p.add_run('*')\n","                            if daihyobuntanList[nameList.index(nm)]=='D':\n","                                p.add_run(nm).underline = WD_UNDERLINE.DOUBLE\n","                            elif daihyobuntanList[nameList.index(nm)]=='B':\n","                                p.add_run(nm).underline = True\n","                            else:\n","                                p.add_run(nm)\n","                        else:\n","                            p.add_run(nm)\n","                        p.add_run(', ')\n","                    p.add_run(pap['text1'])\n","                    p.add_run(pap['text2'])\n","                    if pap['oudan']:\n","                        #print(pap['oudan'])\n","                        p.add_run(' ('+ pap['oudan']+')').font.color.rgb = RGBColor(0,0,255)\n","                        if pap['oudan'] in OudanDict.keys():\n","                            OudanDict[pap['oudan']]['count']+=1\n","                        else:\n","                            OudanDict[pap['oudan']]={}\n","                            OudanDict[pap['oudan']]['count']=1\n","                p=document.add_paragraph('...(計')\n","                p.add_run(str(inds)).font.color.rgb= RGBColor(255,0,0)\n","                p.add_run('件)')\n","                document.add_paragraph('')\n","                ws[papercelllist[i]]=inds\n","                i=i+1\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('共同研究リスト')\n","    DKtext=['大学','企業・公共団体']\n","    kokunaikaigaitext=['国内','海外']\n","\n","    inputpositionsKyodo=['c'+'{0:02d}'.format(i) for i in range(24,32)]\n","    outputpositionsKyodo=[col+'{0:02d}'.format(i) for col in ['C','E'] for i in range(47,51) ]\n","    for iK,oK in zip(inputpositionsKyodo,outputpositionsKyodo):\n","        ws[oK].value=name_dataform[iK].astype(float).sum()\n","\n","    inputpositionsKyodo=['c'+'{0:02d}'.format(i) for i in range(32,40)]\n","    outputpositionsKyodo=[col+'{0:02d}'.format(i) for col in ['D','F'] for i in range(47,51) ]\n","    printtext=[yt + ' '+ kt +' '+ dk + ' '+ '契約書なし' for yt in yearstext  for dk in DKtext for kt in kokunaikaigaitext]\n","\n","    for iK,oK,ptxt in zip(inputpositionsKyodo,outputpositionsKyodo,printtext):\n","        sumKyodo=0\n","        document.add_paragraph(ptxt)\n","        for a,piname in zip(name_dataform[iK],name_dataform['c02']):\n","            if (type(a)==str): \n","                if (len(a)>4): # なし、0などを省くために5文字以上\n","                    b=re.split('[、。,]', a)\n","                    sumKyodo+=len(b)\n","                    p =document.add_paragraph(piname+' 研: ')\n","                    for bb in b:\n","                        p.add_run(bb).underline = True\n","                        p.add_run('　')\n","\n","        ws[oK].value=sumKyodo\n","        p=document.add_paragraph('...(計')\n","        p.add_run(str(sumKyodo)).font.color.rgb= RGBColor(255,0,0)\n","        p.add_run('件)')\n","        document.add_paragraph('')\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","\n","    document.add_paragraph('受賞リスト')\n","\n","    keys=list(AwardsDict.keys())\n","    datelist=[AwardsDict[r]['award_date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    inds=0\n","    for r in arg:\n","        if (AwardsDict[keys[r]]['award_date']>globalmindate) & (AwardsDict[keys[r]]['award_date']<globalmaxdate):\n","            inds=inds+1\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(AwardsDict[keys[r]]['name'])\n","            p.add_run(', '+AwardsDict[keys[r]]['award_name'])\n","            p.add_run(', '+AwardsDict[keys[r]]['association'])\n","            p.add_run(', '+AwardsDict[keys[r]]['award_date']+'.')\n","\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('国際会議招待講演リスト')\n","    keys=list(TalksDict.keys())\n","    datelist=[TalksDict[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    inds=0\n","    for r in arg:\n","        if TalksDict[keys[r]][\"international\"] & (TalksDict[keys[r]]['date']>globalmindate) & (TalksDict[keys[r]]['date']<globalmaxdate) & (TalksDict[keys[r]][\"invited\"] | (TalksDict[keys[r]][\"keyoral\"] in ['invited_oral_presentation','keynote_oral_presentation','nominated_symposium'])):\n","            inds=inds+1\n","            pap=TalksDict[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap[\"presenter\"]\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(', \\\"'+pap[\"presentation_title\"]+\"\\\"\")\n","            p.add_run(' ('+pap[\"keyoral\"]+')').font.color.rgb= RGBColor(255,0,0)\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(', '+pap[\"date\"]+'.')\n","\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('アウトリーチリスト')\n","    keys=list(SocialContDict.keys())\n","    datelist=[SocialContDict[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","\n","    inds=0\n","    for r in arg:\n","        pap=SocialContDict[keys[r]]\n","        if (SocialContDict[keys[r]]['date']>globalmindate) & (SocialContDict[keys[r]]['date']<globalmaxdate):\n","            inds=inds+1\n","            pap=SocialContDict[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(pap[\"name\"])\n","            p.add_run(', '+pap[\"title\"])\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","            \n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('報道リスト')\n","    keys=list(medDict.keys())\n","    datelist=[medDict[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","\n","    inds=0\n","    for r in arg:\n","        if (medDict[keys[r]]['date']>globalmindate) & (medDict[keys[r]]['date']<globalmaxdate):\n","            inds=inds+1\n","            pap=medDict[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(','+pap[\"paper_title\"])\n","            p.add_run(pap[\"publication_name\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('就職状況リスト')\n","\n","    for s in name_dataform['c22']:\n","        if type(s) is str:\n","            p = document.add_paragraph(s)\n","\n","    wb.save(file_name_xlsx)\n","    document.save(file_name_check)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["globalmindate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if ('google.colab' in str(get_ipython())):\n","    files.download(file_name)\n","    if sankodata:\n","        files.download(file_name_xlsx)\n","        files.download(file_name_check)"]}],"metadata":{"colab":{"name":"researchmapv2_to_docx.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"colors":{"hover_highlight":"#DAA520","navigate_num":"#000000","navigate_text":"#333333","running_highlight":"#FF0000","selected_highlight":"#FFD700","sidebar_border":"#EEEEEE","wrapper_background":"#FFFFFF"},"moveMenuLeft":true,"nav_menu":{"height":"12px","width":"252px"},"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":4,"toc_cell":false,"toc_section_display":"block","toc_window_display":false,"widenNotebook":false},"metadata":{"interpreter":{"hash":"b6aaa8c452f1aa53401d42c477fe97baeea39d8952471e90cdbb23434ceb6c90"}},"interpreter":{"hash":"b6aaa8c452f1aa53401d42c477fe97baeea39d8952471e90cdbb23434ceb6c90"}},"nbformat":4,"nbformat_minor":0}