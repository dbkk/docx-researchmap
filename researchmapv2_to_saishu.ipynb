{"cells":[{"cell_type":"code","execution_count":138,"metadata":{"colab":{},"colab_type":"code","id":"a3PnsyICVY2v"},"outputs":[],"source":["## いじるのはこのセルのパラメータのみでOK\n","\n","file_name_download = \"IPB-final.docx\" #ダウンロードされる業績リストのファイル名\n","file_name_download_xlsx = \"IPB-final.xlsx\" #ダウンロードされるxlsxファイルのファイル名\n","file_name_download_check = \"IPB-final_check.docx\" #集計チェック用のファイル名\n","\n","globalmindate='2019-06-28' #これより後の業績を集める\n","globalmaxdate='2025-03-31' #これより前の業績を集める\n","smark='◎' #researchmapで課題番号紐づけありの論文にマーク付ける場合はここで指定。\n","ryoiki_linked = False #researchmapで課題番号紐づけありの論文のみ出力したい場合はTrue\n","allenglish = False #名前表記をすべて英語で統一する場合はTrue, 論文以外の名前表記を日本語にする場合False\n","SNfirst = False #英語名前表記をすべて名字先で統一する場合はTrue, 名字後で統一する場合はFalse\n","numberingPapers = True #出力の際に論文をナンバリング\n","peer_reviewed = True #査読ありのチェックが入った論文だけに限定する場合はTrue\n","firstnameInitial = True\n","\n","sankodata=True # xlsxファイルの読み書きをする場合\n","\n","docoutputpointsize=11 #11pt出力指定\n","\n","# 領域メンバーの情報入りスプレッドシートのURL\n","sheeturl='https://docs.google.com/spreadsheets/d/1wce1XHSFGSBttupnSIqe_5abtijBb_hBYM2bfaV9Jn4/edit#gid=0' \n","\n","# 未入力のxlsxファイルのURL (githubにアップロード済)\n","blankxlsx='https://github.com/dbkk/docx-researchmap/blob/rev2024/inputfiles/R6事後評価報告書（13参考データExcel版）.xlsx'\n","\n","# google formで取ったそれ以外のデータ (非公開) (readme.md参照)\n","sheeturlform='https://docs.google.com/spreadsheets/d/1GsHBU62c63-4F7b20rBdvGpqJXw8gR_Iac3Hw4hVSZI/edit?usp=sharing'\n","\n","#出力時の上限数\n","maxpap=999#9\n","maxtalk=999#5\n","maxsocial=999#5\n","maxmed=999#5\n","maxsonota=999#3\n","maxBSM=999#10"]},{"cell_type":"code","execution_count":139,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"colab_type":"code","executionInfo":{"elapsed":3508,"status":"ok","timestamp":1584094456008,"user":{"displayName":"深井洋佑","photoUrl":"","userId":"07275761346057679589"},"user_tz":-540},"id":"0rIZIv9QVY3G","outputId":"d2e32d75-856a-4695-b1b6-858a7b64442e"},"outputs":[],"source":["import requests,json,sys,os,gspread,time,re,openpyxl,datetime,xlrd\n","import numpy as np\n","import pandas as pd\n","\n","if 'google.colab' in str(get_ipython()):\n","    %pip install python-docx\n","    from google.colab import files,auth\n","    from oauth2client.client import GoogleCredentials\n","    outputdirectory = ''\n","else:\n","    outputdirectory = '../docx-researchmap-outputs/' #ローカルで実行する場合は保存ファイルのディレクトリを適当に指定\n","    os.makedirs(outputdirectory,exist_ok=True)\n","from docx import Document\n","from docx.shared import Pt,Mm,RGBColor\n","from docx.enum.text import WD_UNDERLINE,WD_LINE_SPACING,WD_BREAK\n","\n","file_name=outputdirectory+file_name_download\n","file_name_xlsx=outputdirectory+file_name_download_xlsx\n","file_name_check=outputdirectory+file_name_download_check"]},{"cell_type":"code","execution_count":140,"metadata":{"colab":{},"colab_type":"code","id":"nlnwDJeCW4PW"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>班</th>\n","      <th>番号</th>\n","      <th>代表分担協力</th>\n","      <th>Surname</th>\n","      <th>First name</th>\n","      <th>苗字</th>\n","      <th>名</th>\n","      <th>researchmapID</th>\n","      <th>grantID</th>\n","      <th>Start date</th>\n","      <th>End date</th>\n","      <th>著者名（2個目）</th>\n","      <th>著者名（3個目）</th>\n","      <th>Unnamed: 13</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>1</td>\n","      <td>D</td>\n","      <td>Okada</td>\n","      <td>Yasushi</td>\n","      <td>岡田</td>\n","      <td>康志</td>\n","      <td>yokadayokada</td>\n","      <td>19H05795</td>\n","      <td>2019-06-28</td>\n","      <td>2025-03-31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A</td>\n","      <td>1</td>\n","      <td>B</td>\n","      <td>Sasa</td>\n","      <td>Shin-ichi</td>\n","      <td>佐々</td>\n","      <td>真一</td>\n","      <td>sasa3341</td>\n","      <td>19H05795</td>\n","      <td>2019-06-28</td>\n","      <td>2025-03-31</td>\n","      <td>Sasa Shin-ishi</td>\n","      <td>Sasa Shin-Ichi</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A</td>\n","      <td>1</td>\n","      <td>K</td>\n","      <td>Dechant</td>\n","      <td>Andreas</td>\n","      <td>Dechant</td>\n","      <td>Andreas</td>\n","      <td>adechant</td>\n","      <td>19H05795</td>\n","      <td>2019-06-28</td>\n","      <td>2025-03-31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>1</td>\n","      <td>B</td>\n","      <td>Kawaguchi</td>\n","      <td>Kyogo</td>\n","      <td>川口</td>\n","      <td>喬吾</td>\n","      <td>kyogok</td>\n","      <td>19H05795</td>\n","      <td>2019-06-28</td>\n","      <td>2025-03-31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A</td>\n","      <td>1</td>\n","      <td>B</td>\n","      <td>Kambara</td>\n","      <td>Taketoshi</td>\n","      <td>神原</td>\n","      <td>丈敏</td>\n","      <td>kambara</td>\n","      <td>19H05795</td>\n","      <td>2019-06-28</td>\n","      <td>2025-03-31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>D</td>\n","      <td>2</td>\n","      <td>D</td>\n","      <td>Matsusaki</td>\n","      <td>Motonori</td>\n","      <td>松崎</td>\n","      <td>元紀</td>\n","      <td>matsusaki_motonori</td>\n","      <td>22H04847</td>\n","      <td>2022-05-19</td>\n","      <td>2025-03-31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>D</td>\n","      <td>3</td>\n","      <td>D</td>\n","      <td>Yamada</td>\n","      <td>Hiroshi</td>\n","      <td>山田</td>\n","      <td>洋</td>\n","      <td>read0104380</td>\n","      <td>22H04832</td>\n","      <td>2022-03-31</td>\n","      <td>2025-03-31</td>\n","      <td>Yamada, Hiroshi</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>D</td>\n","      <td>3</td>\n","      <td>D</td>\n","      <td>Toyoshima</td>\n","      <td>Yu</td>\n","      <td>豊島</td>\n","      <td>有</td>\n","      <td>Yu_Toyoshima</td>\n","      <td>22H04838</td>\n","      <td>2022-03-31</td>\n","      <td>2025-03-31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>D</td>\n","      <td>3</td>\n","      <td>D</td>\n","      <td>Shimamura</td>\n","      <td>Teppei</td>\n","      <td>島村</td>\n","      <td>徹平</td>\n","      <td>teppei_shimamura</td>\n","      <td>22H04839</td>\n","      <td>2022-03-31</td>\n","      <td>2025-03-31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>D</td>\n","      <td>3</td>\n","      <td>D</td>\n","      <td>Tsukada</td>\n","      <td>Yuki</td>\n","      <td>塚田</td>\n","      <td>祐基</td>\n","      <td>tsukadayuki</td>\n","      <td>22H04840</td>\n","      <td>2022-03-31</td>\n","      <td>2025-03-31</td>\n","      <td>Yuki Tsukada</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>61 rows × 14 columns</p>\n","</div>"],"text/plain":["    班  番号 代表分担協力    Surname First name       苗字         名       researchmapID  \\\n","0   A   1      D      Okada    Yasushi       岡田        康志        yokadayokada   \n","1   A   1      B       Sasa  Shin-ichi       佐々        真一            sasa3341   \n","2   A   1      K    Dechant    Andreas  Dechant  Andreas             adechant   \n","3   A   1      B  Kawaguchi      Kyogo       川口        喬吾              kyogok   \n","4   A   1      B    Kambara  Taketoshi       神原        丈敏             kambara   \n",".. ..  ..    ...        ...        ...      ...       ...                 ...   \n","65  D   2      D  Matsusaki   Motonori       松崎        元紀  matsusaki_motonori   \n","66  D   3      D     Yamada    Hiroshi       山田         洋         read0104380   \n","68  D   3      D  Toyoshima         Yu       豊島         有        Yu_Toyoshima   \n","69  D   3      D  Shimamura     Teppei       島村        徹平    teppei_shimamura   \n","70  D   3      D    Tsukada      Yuki        塚田        祐基         tsukadayuki   \n","\n","     grantID  Start date    End date         著者名（2個目）        著者名（3個目）  \\\n","0   19H05795  2019-06-28  2025-03-31              NaN             NaN   \n","1   19H05795  2019-06-28  2025-03-31   Sasa Shin-ishi  Sasa Shin-Ichi   \n","2   19H05795  2019-06-28  2025-03-31              NaN             NaN   \n","3   19H05795  2019-06-28  2025-03-31              NaN             NaN   \n","4   19H05795  2019-06-28  2025-03-31              NaN             NaN   \n","..       ...         ...         ...              ...             ...   \n","65  22H04847  2022-05-19  2025-03-31              NaN             NaN   \n","66  22H04832  2022-03-31  2025-03-31  Yamada, Hiroshi             NaN   \n","68  22H04838  2022-03-31  2025-03-31              NaN             NaN   \n","69  22H04839  2022-03-31  2025-03-31              NaN             NaN   \n","70  22H04840  2022-03-31  2025-03-31     Yuki Tsukada             NaN   \n","\n","   Unnamed: 13  \n","0          NaN  \n","1          NaN  \n","2          NaN  \n","3          NaN  \n","4          NaN  \n","..         ...  \n","65         NaN  \n","66         NaN  \n","68         NaN  \n","69         NaN  \n","70         NaN  \n","\n","[61 rows x 14 columns]"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["#スプレッドシートをダウンロード\n","sheeturl_csv=re.match(\"https://docs.google.com/spreadsheets/d/.+/\",sheeturl).group(0)+\"export?format=csv\"\n","name_data=pd.read_csv(sheeturl_csv)\n","name_data\n","\n","# remove the duplicate researchmapID and keep the first one\n","name_data = name_data.drop_duplicates(subset='researchmapID', keep='first')\n","\n","name_data"]},{"cell_type":"code","execution_count":141,"metadata":{"colab":{},"colab_type":"code","id":"1E8b1HjY3oN-"},"outputs":[],"source":["membernum=len(name_data)\n","\n","if SNfirst:\n","    allnames=(name_data[\"Surname\"]+' '+name_data[\"First name\"]).to_list()\n","else:\n","    allnames=(name_data[\"First name\"]+' '+name_data[\"Surname\"]).to_list()\n","allSurname=name_data[\"Surname\"].to_list()\n","allnamesJP=(name_data[\"苗字\"]+\" \"+name_data[\"名\"]).to_list()\n","allgroupnames=name_data[\"班\"].to_list()\n","allgroupnum=name_data[\"番号\"].to_list()\n","allmembers=name_data[\"researchmapID\"].to_list()\n","allDB=name_data[\"代表分担協力\"].values\n","allkeikaku=[b for a,b in zip(allgroupnames,allnamesJP) if a in ['A','B','C']]\n","allkeikakuPIs=[b for a,b,c in zip(allgroupnames,allnamesJP,allDB) if (a in ['A','B','C']) & (c =='D')]\n","allDaihyoBuntan=list(allDB)\n","allHan=(name_data[\"班\"]+name_data[\"番号\"].apply(str)).to_list()\n","grant_numbers=name_data[\"grantID\"].to_list()\n","allmindate=name_data[\"Start date\"].to_list()\n","allmaxdate=name_data[\"End date\"].to_list()\n","\n","#Exception names handling\n","altname2,altname3=name_data['著者名（2個目）'],name_data['著者名（3個目）']\n","arraltname2,arraltname3=altname2.values,altname3.values\n","nameList=allnames+list(arraltname2[~(pd.isna(altname2).values)])+list(arraltname3[~(pd.isna(altname3).values)])\n","nameList = [n.strip() for n in nameList]\n","daihyobuntanList=allDaihyoBuntan+list(allDB[~(pd.isna(altname2).values)])+list(allDB[~(pd.isna(altname3).values)])\n","\n"]},{"cell_type":"code","execution_count":142,"metadata":{"colab":{},"colab_type":"code","id":"FNRoF4CLh0lq"},"outputs":[],"source":["# Function to set the name order\n","def SurnameFirst(namesDic,sn):\n","    oldnamelist=[]\n","    swap=0\n","    for indiv in namesDic:\n","        oldnamelist=oldnamelist+[indiv['name'].replace(',','').replace('.','')]\n","        #print(oldnamelist)\n","    return SurnameFirstList(oldnamelist,sn)\n","\n","def SurnameFirstList(oldnamelist,sn):\n","    swap=0\n","    for name in oldnamelist:\n","        if sn in name.split(' '):\n","            if name.split(' ').index(sn)==0: # surname first\n","                swap= True ^ SNfirst\n","                break;\n","            else:\n","                swap= False ^ SNfirst\n","                break;\n","    if swap:\n","        newnamelist=[]\n","        for name in oldnamelist:\n","            namesplit=name.split(' ')\n","            names=[namesplit[-1]]+namesplit[:-1]\n","            newnamelist=newnamelist+[' '.join(names)]\n","    else:\n","        newnamelist=oldnamelist\n","    \n","    if SNfirst & firstnameInitial:\n","        holdlist=[]\n","        for name in newnamelist:\n","            namesplit=name.split(' ')\n","            names=[namesplit[0]]+[', ']+[namesplit[1][0]]+['.']\n","            holdlist=holdlist+[''.join(names)]\n","        newnamelist=holdlist\n","    elif firstnameInitial:\n","        holdlist=[]\n","        for name in newnamelist:\n","            namesplit=name.split(' ')\n","            sn=namesplit[-1]\n","            sn=sn.lower()\n","            sn=sn[0].upper()+sn[1:]\n","            names=[namesplit[0][0]]+['. ']+[sn]\n","            holdlist=holdlist+[''.join(names)]\n","        newnamelist=holdlist                    \n","    return newnamelist\n","\n","def ReturnDictWOerror(dictdata,key,nodata):\n","    if key in dictdata.keys():\n","        return dictdata[key]\n","    else:\n","        return nodata\n","\n","def ReturnDictContent(dictdata,key,key1,nodata=''):\n","    d=ReturnDictWOerror(dictdata,key,nodata)\n","    d1=ReturnDictWOerror(dictdata,key1,nodata)\n","    if d!=nodata:\n","        return d\n","    else:\n","        return d1\n","\n","def commaR(vol,spage):\n","    if (vol=='') & (spage==''):\n","        return ''\n","    elif (vol=='') | (spage==''):\n","        return ' '\n","    else:\n","        return ', '"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[],"source":["# url = \"https://api.researchmap.jp/\"\n","# itemslist = [\"published_papers\",\"research_projects\",\"misc\",\"presentations\",\"books_etc\",\"social_contribution\",\"awards\",\"media_coverage\"]\n","# jsonfiles={}\n","\n","# for name in allmembers:\n","#     print('downloading: '+name)\n","#     jsonfiles[name]={}\n","#     for it in itemslist:\n","#         r1 = requests.get(url+name+'/'+it)\n","#         jsonfiles[name][it]=json.loads(r1.text)\n","#         if 'error' in jsonfiles[name][it].keys():\n","#             print(jsonfiles[name][it]['error'])\n","#             print(\"  error in:\"+it)"]},{"cell_type":"code","execution_count":144,"metadata":{"colab":{},"colab_type":"code","id":"96xz8YVTh0ly"},"outputs":[],"source":["# make dictionary of all papers\n","i=0\n","PapersDict={}\n","\n","doilist=[]\n","doiDict={}\n","titlelist=[]\n","titleDict={}\n","#dc = Document()\n","for ids,fullname,dh,mindate,maxdate,han in zip(allmembers,allnames,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","    surname=fullname.split(' ')[0 if SNfirst else 1]\n","    dfP = jsonfiles[ids][\"published_papers\"]\n","    dfG = jsonfiles[ids][\"research_projects\"]\n","    if 'items' in dfG.keys():\n","        grantID=\"0\"\n","        for dfs in dfG['items']:\n","            if 'identifiers' in dfs.keys():\n","                if 'grant_number' in dfs['identifiers'].keys():\n","                    if dfs['identifiers']['grant_number'][0] in grant_numbers:\n","                        grantID=dfs['rm:id']\n","                        break\n","    if 'items' in dfP.keys():    \n","        for dfs in dfP['items']:\n","            if \"authors\" not in dfs.keys():\n","                continue\n","            if ('identifiers' in dfs.keys()) & (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","                doinum=[0]\n","                if 'doi' in dfs['identifiers'].keys():\n","                    doinum=dfs['identifiers']['doi']\n","\n","                PapersDict[i]={}\n","                PapersDict[i]['issues']=False\n","                PapersDict[i]['preprint']=False\n","                correspo=False\n","                Ryoiki=False\n","                if 'rm:research_project_id' in dfs['identifiers'].keys():\n","                    if grantID in dfs['identifiers']['rm:research_project_id']:\n","                        Ryoiki=True\n","                        \n","                if \"published_paper_owner_roles\" in dfs.keys():\n","                    if (\"corresponding\" in dfs[\"published_paper_owner_roles\"]) | (\"last\" in dfs[\"published_paper_owner_roles\"]):\n","                        correspo=True\n","\n","                jname=''        \n","                if \"publication_name\" in dfs.keys():\n","                    jname=ReturnDictContent(dfs[\"publication_name\"],'en','ja','').upper()\n","\n","                if jname =='ARXIV':\n","                    PapersDict[i]['preprint']=True\n","                    if \"arxiv_id\" in dfs['identifiers'].keys():\n","                        jname=dfs['identifiers']['arxiv_id'][0] + ' (preprint)'\n","                    else:\n","                        jname='arxiv'\n","                \n","                if not(\"publication_name\" in dfs.keys()):\n","                    if \"arxiv_id\" in dfs['identifiers'].keys():\n","                        jname=dfs['identifiers']['arxiv_id'][0] + ' (preprint)'\n","                        PapersDict[i]['preprint']=True\n","                    elif doinum[0]!=0:\n","                        jname='DOI: '+doinum[0]\n","                        PapersDict[i]['preprint']=True\n","                    else:\n","                        jname='journal unspecified'\n","                        PapersDict[i]['issues']=True\n","                    \n","                Sname=SurnameFirst(ReturnDictContent(dfs[\"authors\"],'en','ja',''),surname)\n","\n","                spage=''\n","                if \"starting_page\" in dfs.keys():\n","                    if dfs[\"starting_page\"]!='':\n","                        spage=dfs[\"starting_page\"]\n","\n","                vol=''\n","                if \"volume\" in dfs.keys():\n","                    if dfs[\"volume\"]!='':\n","                        vol=' '+dfs[\"volume\"]\n","                if doinum in doilist:\n","                    doiDict[doinum[0]]['name']=doiDict[doinum[0]]['name']+[fullname]\n","                    doiDict[doinum[0]]['Corresp']=doiDict[doinum[0]]['Corresp']+[correspo]\n","                else:\n","                    doiDict[doinum[0]]={}\n","                    doiDict[doinum[0]]['name']=[fullname]\n","                    doiDict[doinum[0]]['Corresp']=[correspo]\n","                    doiDict[doinum[0]]['count']=0\n","                    doilist=doilist+[doinum[0]]\n","                \n","                papertitle=ReturnDictContent(dfs['paper_title'],'en','ja','')\n","                papid=papertitle.upper().rstrip('.')\n","\n","                if papid in titlelist:\n","                    titleDict[papid]['name'] = titleDict[papid]['name']+[fullname]\n","                    titleDict[papid]['Corresp'] = titleDict[papid]['Corresp']+[correspo]                    \n","                else:\n","                    titlelist = titlelist + [papid]\n","                    titleDict[papid] = {}\n","                    titleDict[papid]['name'] = [fullname]\n","                    titleDict[papid]['Corresp'] = [correspo]\n","                    titleDict[papid]['count']=0\n","\n","                text1=\"\\\"\"+papertitle+\"\\\"\" +', '\n","                text2=jname+','+vol+commaR(vol,spage)+spage+ ' ('+dfs[\"publication_date\"][:4] +').'\n","                if \"description\" in dfs.keys():\n","                    PapersDict[i]['oudan']=ReturnDictWOerror(dfs[\"description\"],'ja','')\n","                else:\n","                    PapersDict[i]['oudan']=''\n","                #print(PapersDict[i]['oudan'])\n","                PapersDict[i]['kokunai']=False\n","                if \"is_international_journal\" in dfs.keys():\n","                    if not dfs[\"is_international_journal\"]:\n","                        PapersDict[i]['kokunai']=True\n","                PapersDict[i]['text1']=text1\n","                PapersDict[i]['text2']=text2\n","                PapersDict[i]['papid']=papid\n","                PapersDict[i]['researcher']=fullname\n","                PapersDict[i]['authors']=Sname\n","                PapersDict[i]['date']=dfs[\"publication_date\"]\n","                PapersDict[i]['referee']=ReturnDictContent(dfs,'referee','referee',False)\n","                PapersDict[i]['doi']=doinum[0]\n","                PapersDict[i]['ryoiki']=Ryoiki\n","                PapersDict[i]['Daihyo']=dh\n","                PapersDict[i]['han']=han\n","                PapersDict[i]['Corresp']=correspo\n","                i=i+1"]},{"cell_type":"code","execution_count":145,"metadata":{"colab":{},"colab_type":"code","id":"cw-jxR49h0l1","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["7000010388\n"]}],"source":["# make dictionary of all talks, 'social_contribution', awards\n","TalksDict={}\n","SocialContDict={}\n","AwardsDict={}\n","i,j,l=0,0,0\n","for ids,fullname,fullnameJP,dh,mindate,maxdate,han in zip(allmembers,allnames,allnamesJP,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","    dfPr = jsonfiles[ids][\"presentations\"]\n","    dfSC = jsonfiles[ids][\"social_contribution\"]\n","    dfAw = jsonfiles[ids][\"awards\"]\n","    # remove duplicates in presentations\n","    dfPr['items'] = [i for n, i in enumerate(dfPr['items']) if i not in dfPr['items'][n + 1:]]\n","\n","    if 'items' in dfPr.keys():\n","        for dfs in dfPr['items']:\n","            if all([a in dfs.keys() for a in [\"presentation_title\",\"event\",'publication_date','presenters']]):\n","                if (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","                    if ('en' in dfs[\"presenters\"].keys()):\n","                        pname=dfs[\"presenters\"][\"en\"][0][\"name\"]\n","                    else:\n","                        pname=dfs[\"presenters\"][\"ja\"][0][\"name\"]\n","                    ename=ReturnDictContent(dfs[\"event\"],'ja','en','')\n","                    ptitle=ReturnDictContent(dfs[\"presentation_title\"],'ja','en','')\n","                    pdate=dfs[\"publication_date\"]\n","                    TalksDict[i]={}\n","                    TalksDict[i][\"presenter\"]=fullnameJP\n","                    if allenglish:\n","                        TalksDict[i]['printname']=fullname\n","                    else:\n","                        TalksDict[i][\"printname\"]=fullnameJP\n","                    TalksDict[i][\"event\"]=ename\n","                    TalksDict[i][\"presentation_title\"]=ptitle\n","                    TalksDict[i][\"date\"]=pdate\n","                    TalksDict[i][\"han\"]=han\n","                    TalksDict[i][\"invited\"]=ReturnDictWOerror(dfs,'invited',False)\n","                    TalksDict[i][\"international\"]=ReturnDictWOerror(dfs,'is_international_presentation',False)\n","                    TalksDict[i][\"keyoral\"]= ReturnDictWOerror(dfs,\"presentation_type\",'')\n","                    if \"Force transmission via retrograde\" in ptitle:\n","                        print(ids)\n","                    i=i+1\n","    if 'items' in dfSC.keys():\n","        for dfs in dfSC['items']:\n","            if 'from_event_date' in dfs.keys():\n","                if (dfs['from_event_date']>=mindate) & (dfs['from_event_date']<=maxdate):\n","                    SocialContDict[j]={}\n","                    SocialContDict[j]['name']=fullnameJP\n","                    SocialContDict[j][\"title\"]=dfs['social_contribution_title']['ja']\n","                    SocialContDict[j][\"date\"]=dfs['from_event_date']\n","                    SocialContDict[j][\"han\"]=han\n","                    if 'event' in dfs.keys():\n","                        SocialContDict[j][\"event\"]=ReturnDictContent(dfs[\"event\"],'ja','en','')\n","                    else:\n","                        SocialContDict[j][\"event\"]=''\n","                    j=j+1\n","    if 'items' in dfAw.keys():\n","        for dfs in dfAw['items']:\n","            if (dfs['award_date']>=mindate) & (dfs['award_date']<=maxdate):\n","                AwardsDict[l]={}\n","                AwardsDict[l]['name']=fullnameJP\n","                AwardsDict[l]['award_name']=ReturnDictContent(dfs['award_name'],'ja','en','')\n","                if 'association' in dfs.keys():\n","                    AwardsDict[l]['association']=ReturnDictContent(dfs['association'],'ja','en','')\n","                else:\n","                    AwardsDict[l]['association']=''\n","                AwardsDict[l]['award_date']=dfs['award_date']\n","                l=l+1"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["# find duplicates in allmembers\n","\n","set_m=set(allmembers)\n","duplist=[]\n","for i in set_m:\n","    if allmembers.count(i)>1:\n","        duplist=duplist+[i]\n","print(duplist)\n"]},{"cell_type":"code","execution_count":147,"metadata":{"colab":{},"colab_type":"code","id":"hctp5YUnh0l4"},"outputs":[],"source":["# make dictionary of all books_etc\n","booksDict={}\n","i=0\n","for ids,fullname,fullnameJP,dh,mindate,maxdate,han in zip(allmembers,allnames,allnamesJP,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","  dfM = jsonfiles[ids][\"books_etc\"]\n","  if 'items' in dfM.keys():\n","    for dfs in dfM['items']:\n","      if all([a in dfs.keys() for a in ['authors',\"book_title\",\"publication_date\"]]):\n","        if (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","          if ('ja' in dfs[\"authors\"].keys()):\n","              pname=dfs[\"authors\"][\"ja\"][0][\"name\"]\n","          else:\n","              pname=dfs[\"authors\"][\"en\"][0][\"name\"]\n","          ename=ReturnDictContent(dfs[\"book_title\"],'ja','en','')\n","          if \"book_owner_range\" in dfs.keys():\n","            eoname=\" \\'\"+ReturnDictContent(dfs[\"book_owner_range\"],'ja','en','')+\"\\',\"\n","          else:\n","            eoname=''\n","          if \"book_owner_role\" in dfs.keys():\n","            brole=\" (\"+dfs[\"book_owner_role\"]+\"),\"\n","          else:\n","            brole=','\n","          if \"publisher\" in dfs.keys():\n","            pub=\" \"+ReturnDictContent(dfs[\"publisher\"],'ja','en','')+\",\"\n","          else:\n","            pub=''\n","          pdate=dfs[\"publication_date\"]\n","          booksDict[i]={}\n","          booksDict[i]['authors']=fullname\n","          if allenglish:\n","            booksDict[i]['printname']=fullname\n","          else:\n","            booksDict[i]['printname']=fullnameJP\n","          booksDict[i][\"book_title\"]=' '+ename+','\n","          booksDict[i][\"book_owner_role\"]=brole\n","          booksDict[i][\"book_owner_range\"]=eoname\n","          booksDict[i][\"publisher\"]=pub\n","          booksDict[i][\"date\"]=pdate\n","          booksDict[i][\"han\"]=han\n","          i=i+1"]},{"cell_type":"code","execution_count":148,"metadata":{"colab":{},"colab_type":"code","id":"nFe7cnwg6qNv"},"outputs":[],"source":["# make dictionary of all MISCs\n","miscDict={}\n","medDict={}\n","i,j=0,0\n","for ids,fullname,fullnameJP,dh,mindate,maxdate,han in zip(allmembers,allnames,allnamesJP,allDaihyoBuntan,allmindate,allmaxdate,allHan):\n","  dfMis = jsonfiles[ids][\"misc\"]\n","  if 'items' in dfMis.keys():\n","    for dfs in dfMis['items']:\n","      if all([a in dfs.keys() for a in ['authors',\"paper_title\",\"publication_date\",\"publication_name\"]]):\n","        if  (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","          if ('ja' in dfs[\"authors\"].keys()):\n","              pname=dfs[\"authors\"][\"ja\"][0][\"name\"]\n","          else:\n","              pname=dfs[\"authors\"][\"en\"][0][\"name\"]\n","          ename=ReturnDictContent(dfs[\"paper_title\"],'ja','en','')\n","          ptitle=ReturnDictContent(dfs[\"publication_name\"],'ja','en','')\n","          pdate=dfs[\"publication_date\"]\n","          miscDict[i]={}\n","          miscDict[i]['authors']=fullname\n","          if allenglish:\n","            miscDict[i]['printname']=fullname\n","          else:\n","            miscDict[i]['printname']=fullnameJP\n","\n","          miscDict[i][\"paper_title\"]=' \\''+ename+'\\','\n","          miscDict[i][\"publication_name\"]=' '+ptitle+','\n","          miscDict[i][\"date\"]=pdate\n","          miscDict[i][\"han\"]=han\n","          i=i+1\n","  dfMed = jsonfiles[ids][\"media_coverage\"]\n","  if 'items' in dfMed.keys():\n","    for dfs in dfMed['items']:\n","      if all([a in dfs.keys() for a in [\"media_coverage_title\",\"publication_date\"]]):\n","        if  (dfs[\"publication_date\"]>=mindate) & (dfs[\"publication_date\"]<=maxdate):\n","          ename=ReturnDictContent(dfs[\"media_coverage_title\"],'ja','en','')\n","          \n","          \n","          pdate=dfs[\"publication_date\"]\n","          medDict[j]={}\n","          medDict[j]['authors']=fullname\n","          if allenglish:\n","            medDict[j]['printname']=fullname\n","          else:\n","            medDict[j]['printname']=fullnameJP\n","          medDict[j][\"media_coverage_type\"]=''\n","          if \"media_coverage_type\" in dfs.keys():\n","            medDict[j][\"media_coverage_type\"]=dfs[\"media_coverage_type\"]\n","          ptitle=''\n","          if \"publisher\" in dfs.keys():\n","            ptitle=ReturnDictContent(dfs[\"publisher\"],'ja','en','')+', '\n","\n","          petitle=''\n","          if \"event\" in dfs.keys():\n","            petitle=ReturnDictContent(dfs[\"event\"],'ja','en','')+','\n","#          print(' '+ptitle+petitle)\n","\n","          pltitle=''\n","          if \"location\" in dfs.keys():\n","            pltitle=ReturnDictContent(dfs[\"location\"],'ja','en','')+','\n","#          print(' '+ptitle+petitle+pltitle)\n","\n","          medDict[j][\"paper_title\"]=' \\''+ename+'\\','\n","          medDict[j][\"publication_name\"]=' '+ptitle+petitle+pltitle\n","          medDict[j][\"date\"]=pdate\n","          medDict[j][\"han\"]=han\n","          j=j+1"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[],"source":["# generate docx\n","document = Document()\n","\n","section = document.sections[0]\n","section.left_margin = Mm(15)\n","section.right_margin = Mm(15)\n","section.top_margin = Mm(15)\n","section.bottom_margin = Mm(15)\n","\n","nameListPrint=SurnameFirstList(nameList,'Okada')\n","\n","if peer_reviewed:\n","    refbool=[True]\n","else:\n","    refbool=[True,False]\n","\n","if ryoiki_linked:\n","    ryoikibool=[True]\n","else:\n","    ryoikibool=[True,False]\n","\n","for han in np.unique(allHan):\n","\n","    PapersDictSelected={k:PapersDict[k] for k in range(len(PapersDict)) if (PapersDict[k]['date']>globalmindate) & (PapersDict[k]['date']<globalmaxdate)  & (PapersDict[k]['referee'] in refbool) & (PapersDict[k]['han']==han) & (PapersDict[k]['ryoiki'] in ryoikibool)}\n","    TalksDictSelected={k:TalksDict[k] for k in range(len(TalksDict)) if (TalksDict[k]['date']>globalmindate) & (TalksDict[k]['date']<globalmaxdate) & (TalksDict[k]['han']==han)}\n","    booksDictSelected={k:booksDict[k] for k in range(len(booksDict)) if (booksDict[k]['date']>globalmindate) & (booksDict[k]['date']<globalmaxdate) & (booksDict[k]['han']==han)}\n","    SocialContDictSelected= {k:SocialContDict[k] for k in range(len(SocialContDict)) if (SocialContDict[k]['date']>globalmindate) & (SocialContDict[k]['date']<globalmaxdate) & (SocialContDict[k]['han']==han)}\n","    miscDictSelected={k:miscDict[k] for k in range(len(miscDict)) if (miscDict[k]['date']>globalmindate) & (miscDict[k]['date']<globalmaxdate) & (miscDict[k]['han']==han)}\n","    medDictSelected={k:medDict[k] for k in range(len(medDict)) if (medDict[k]['date']>globalmindate) & (medDict[k]['date']<globalmaxdate) & (medDict[k]['han']==han)}\n","\n","    keys=list(PapersDictSelected.keys())\n","    datelist=[PapersDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","\n","    document.add_paragraph(han[0]+'0'+han[1]+'班')\n","\n","    countBSM=len(booksDictSelected)+len(SocialContDictSelected)+len(miscDictSelected)\n","\n","    if countBSM < maxBSM:\n","        maxpaps=maxpap+maxBSM-countBSM\n","    else:\n","        maxpaps=maxpap\n","    #print(countBSM)\n","    textmax= ', うち'+str(min(maxpaps,len(arg)))+'件抜粋'# if inds>maxpaps else ''\n","    CountR=document.add_paragraph('<原著論文> 査読有計'+str(len(arg))+'件'+textmax)\n","    CountR.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        pap=PapersDictSelected[keys[r]]\n","        ## to eliminate duplicates of papers\n","        # based on DOI\n","        if len(doiDict[pap['doi']]['name'])>1:\n","            if doiDict[pap['doi']]['count']==1:\n","                continue;\n","            titleDict[pap['papid']]['count']=1\n","            doiDict[pap['doi']]['count']=1\n","        # based on paper title\n","        if (len(titleDict[pap['papid']]['name'])>1):\n","            if titleDict[pap['papid']]['count']==1:\n","                continue;\n","            titleDict[pap['papid']]['count']=1\n","            doiDict[pap['doi']]['count']=1\n","        inds=inds+1\n","        if inds<=maxpaps:\n","            if pap['issues']:\n","                p = document.add_paragraph('***')\n","            if numberingPapers:\n","                if pap['ryoiki']:\n","                    p = document.add_paragraph(smark+str(inds)+'. ')\n","                else:\n","                    p = document.add_paragraph(str(inds)+'. ')\n","            else:\n","                if pap['ryoiki']:\n","                    p = document.add_paragraph(smark)\n","                else:\n","                    p = document.add_paragraph('')\n","\n","            for nm in pap['authors']:\n","                if nm in nameListPrint:\n","                    listedCorrespo = any([c for c,n in zip(doiDict[pap['doi']]['Corresp'] + titleDict[pap['papid']]['Corresp'] , doiDict[pap['doi']]['name'] + titleDict[pap['papid']]['name']) if n==nm])\n","                    # print(nm,listedCorrespo)\n","                    if pap['Corresp'] | listedCorrespo:\n","                        p.add_run('*')\n","                    if daihyobuntanList[nameListPrint.index(nm)]=='D':\n","                        run=p.add_run()\n","                        run.text=nm\n","                        run.underline = WD_UNDERLINE.DOUBLE\n","                        run.font.bold =True\n","                    elif daihyobuntanList[nameListPrint.index(nm)]=='B':\n","                        run=p.add_run()\n","                        run.text=nm\n","                        run.underline = True\n","                        run.font.bold =True\n","                    else:\n","                        p.add_run(nm)\n","                else:\n","                    p.add_run(nm)\n","                p.add_run(', ')\n","            p.add_run(pap['text1'])\n","            p.add_run(pap['text2'])\n","\n","    if inds != len(arg):\n","        textmax= ', うち'+str(min(maxpaps,inds))+'件抜粋'# if inds>maxpaps else ''\n","        CountR.text='<原著論文> 査読有計'+str(inds)+'件'+textmax\n","        CountR.runs[0].bold=True\n","        #replaced_text = paragraph.text.replace(\"before\",\"after\")\n","\n","    for r in keys:\n","        doiDict[PapersDictSelected[r]['doi']]['count']=0\n","        titleDict[PapersDictSelected[r]['papid']]['count']=0\n","\n","\n","    keys=list(TalksDictSelected.keys())\n","    datelist=[TalksDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    textmax= ', うち'+str(min(maxtalk,len(arg)))+'件抜粋'# if len(arg)>maxtalk else ''\n","    CountG=document.add_paragraph('<学会発表・講演> 計'+str(len(arg))+'件'+textmax)\n","    CountG.runs[0].bold=True\n","\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxtalk:\n","            pap=TalksDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap[\"presenter\"]\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(', \\\"'+pap[\"presentation_title\"]+\"\\\"\")\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(', '+pap[\"date\"]+'.')\n","\n","    keys=list(booksDictSelected.keys())\n","    datelist=[booksDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    CountB=document.add_paragraph('<書籍> 計'+str(len(arg))+'件')\n","    CountB.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        pap=booksDictSelected[keys[r]]\n","        p = document.add_paragraph(str(inds)+'. ')\n","        nm=pap['authors']\n","        p.add_run(pap[\"printname\"]) \n","        p.add_run(pap[\"book_owner_role\"])\n","        p.add_run(pap[\"book_owner_range\"])\n","        p.add_run(pap[\"book_title\"])\n","        p.add_run(pap[\"publisher\"])\n","        p.add_run(' '+pap[\"date\"][:7]+'.')\n","\n","    keys=list(SocialContDictSelected.keys())\n","    datelist=[SocialContDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    textmax= ', うち'+str(min(maxsocial,len(arg)))+'件抜粋'# if len(arg)>maxsocial else ''\n","    CountO=document.add_paragraph('<アウトリーチ> 計'+str(len(arg))+'件'+textmax)\n","    CountO.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxsocial:\n","            pap=SocialContDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(pap[\"name\"])\n","            p.add_run(', '+pap[\"title\"])\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","\n","    keys=list(medDictSelected.keys())\n","    datelist=[medDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    textmax= ', うち'+str(min(maxmed,len(arg)))+'件抜粋'# if len(arg)>maxmed else ''\n","    CountP=document.add_paragraph('<報道> 計'+str(len(arg))+'件'+textmax)\n","    CountP.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxsonota:\n","            pap=medDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap['authors']\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(','+pap[\"paper_title\"])\n","            p.add_run(pap[\"publication_name\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","\n","    keys=list(miscDictSelected.keys())\n","    datelist=[miscDictSelected[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    #document.add_paragraph('')\n","    textmax= ', うち'+str(min(maxsonota,len(arg)))+'件抜粋'# if len(arg)>maxsonota else ''\n","    CountM=document.add_paragraph('<その他> 計'+str(len(arg))+'件'+textmax)\n","    CountM.runs[0].bold=True\n","    inds=0\n","    for r in arg:\n","        inds=inds+1\n","        if inds<=maxsonota:\n","            pap=miscDictSelected[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap['authors']\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(','+pap[\"paper_title\"])\n","            p.add_run(pap[\"publication_name\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","    p = document.add_paragraph()\n","\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","for paragraph in document.paragraphs:\n","    paragraph.style = document.styles['Normal']\n","    paragraph.paragraph_format.space_before = Pt(2)\n","    paragraph.paragraph_format.space_after = Pt(2)\n","    for run in paragraph.runs:\n","        run.font.size = Pt(docoutputpointsize)\n","document.save(file_name)"]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":["# 以下「13 参考データ」入力対応\n","if sankodata:\n","    sheeturlform_csv=re.match(\"https://docs.google.com/spreadsheets/d/.+/\",sheeturlform).group(0)+\"export?format=csv\"\n","    col_names = ['c{0:02d}'.format(i) for i in range(54)]\n","    name_dataform=pd.read_csv(sheeturlform_csv,header=0,names=col_names)\n","    name_dataform=name_dataform.sort_values('c00')\n","    name_dataform=name_dataform.drop_duplicates(['c02'],keep='last') # 最新のデータだけ使う\n","    keikakudata=name_dataform[name_dataform['c02'].isin(allkeikaku)]\n","    buntandata=name_dataform[~name_dataform['c02'].isin(allkeikaku)]\n","\n","    timestamps=name_dataform.iloc[:,0].to_list()\n","    namelist=name_dataform.iloc[:,2].to_list()\n","\n","    #keikakudata"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["if sankodata:\n","    FYstart=['2021-04-01','2022-04-01','2023-04-01']\n","    FYend=['2022-03-31','2023-03-31','2024-03-31']\n","    yearcol=['G','I','K']\n","\n","    if 'google.colab' in str(get_ipython()):\n","        import urllib.request\n","        urllib.request.urlretrieve(blankxlsx, 'blankfile.xlsx')\n","        wb = openpyxl.load_workbook(\"blankfile.xlsx\")\n","    else:\n","        wb = openpyxl.load_workbook(\"./inputfiles/R6事後評価報告書（13参考データExcel版）.xlsx\")\n","\n","    ws = wb.worksheets[0]\n","\n","    document = Document()\n","    document.add_paragraph('カウントチェック用出力'+str(datetime.datetime.today()))\n","\n","    section = document.sections[0]\n","    section.left_margin = Mm(15)\n","    section.right_margin = Mm(15)\n","    section.top_margin = Mm(15)\n","    section.bottom_margin = Mm(15)\n","\n","\n","    # 研究代表者人数\n","    ## R3 計画\n","    ws['G8'].value=len(allkeikakuPIs)\n","    ## R4 計画\n","    ws['I8'].value=len(allkeikakuPIs)\n","    ## R5 計画\n","    ws['K8'].value=len(allkeikakuPIs)\n","\n","    ## R3 公募\n","    ws['H8'].value=allgroupnames.count('D')\n","    ## R4 公募\n","    ws['J8'].value=allgroupnames.count('D')\n","    ## R5 公募\n","    ws['L8'].value=allgroupnames.count('D')\n","\n","\n","    # 研究分担者人数\n","    ## R3 計画\n","    ws['G9'].value=np.sum(keikakudata['c03'])\n","    ## R4 計画\n","    ws['I9'].value=np.sum(keikakudata['c09'])\n","    ## R5 計画\n","    ws['K9'].value=np.sum(keikakudata['c15'])\n","\n","    #研究協力者 若手研究者 外国人研究者 ポスドク RA等\n","    keikakulist=[col+'{0:02d}'.format(i) for col in ['G','I','K'] for i in range(10,15) ]\n","    buntanlist=[col+'{0:02d}'.format(i)  for col in ['H','J','L'] for i in range(10,15)]\n","    datapositions=['c'+'{0:02d}'.format(i) for i in [4,5,6,7,8,10,11,12,13,14,16,17,18,19,20]]\n","    for d,kei,bun in zip(datapositions,keikakulist,buntanlist):\n","        ws[kei].value=keikakudata[d].astype(float).sum()\n","        ws[bun].value=buntandata[d].astype(float).sum()\n","\n","    papercelllist=[col+'{0:02d}'.format(i) for col in yearcol for i in range(21,25) ]\n","    yearstext=['令和３年度', '令和４年度', '令和５年度']\n","    years=len(yearstext)\n","\n","    reftext=['査読あり','査読無し']\n","    kokunaitext=['国際誌','国内誌']\n","    OudanDict={}\n","\n","    i=0\n","    for y,yt in zip(range(years),yearstext):\n","        for kokunai,kt in zip([False,True],kokunaitext):\n","            for referee,rt in zip([True,False],reftext):\n","                subdict={k:PapersDict[k] for k in range(len(PapersDict)) if (PapersDict[k]['date']>FYstart[y]) & (PapersDict[k]['date']<=FYend[y])  & (PapersDict[k]['referee'] == referee) & (PapersDict[k]['ryoiki'] in ryoikibool) & (PapersDict[k]['kokunai']==kokunai)}\n","                #print('<'+yt+' '+kt+' '+rt+'>')\n","                document.add_paragraph('<'+yt+' '+kt+' '+rt+'>')\n","                \n","                keys=list(subdict.keys())\n","                datelist=[subdict[r]['date'] for r in keys]\n","                arg=np.argsort(datelist)[::-1]\n","                inds=0\n","                for r in arg:\n","                    \n","                    pap=subdict[keys[r]]\n","                    ## to eliminate duplicates of papers\n","                    # based on DOI\n","                    if len(doiDict[pap['doi']]['name'])>1:\n","                        if doiDict[pap['doi']]['count']==1:\n","                            continue;\n","                        titleDict[pap['papid']]['count']=1\n","                        doiDict[pap['doi']]['count']=1\n","                    # based on paper title\n","                    if (len(titleDict[pap['papid']]['name'])>1):\n","                        if titleDict[pap['papid']]['count']==1:\n","                            continue;\n","                        titleDict[pap['papid']]['count']=1\n","                        doiDict[pap['doi']]['count']=1\n","                    inds=inds+1\n","                    if pap['issues']:\n","                        p = document.add_paragraph('***')\n","                    if pap['ryoiki']:\n","                        p = document.add_paragraph(smark+str(inds)+'. ')\n","                    else:\n","                        p = document.add_paragraph(str(inds)+'. ')\n","                        p.add_run('(紐づけなし) ').font.color.rgb= RGBColor(255,0,0)\n","                    for nm in pap['authors']:\n","                        if nm in nameListPrint:\n","                            listedCorrespo = any([c for c,n in zip(doiDict[pap['doi']]['Corresp'] + titleDict[pap['papid']]['Corresp'] , doiDict[pap['doi']]['name'] + titleDict[pap['papid']]['name']) if n==nm])\n","                            # print(nm,listedCorrespo)\n","                            if pap['Corresp'] | listedCorrespo:\n","                                p.add_run('*')\n","                            if daihyobuntanList[nameListPrint.index(nm)]=='D':\n","                                p.add_run(nm).underline = WD_UNDERLINE.DOUBLE\n","                            elif daihyobuntanList[nameListPrint.index(nm)]=='B':\n","                                p.add_run(nm).underline = True\n","                            else:\n","                                p.add_run(nm)\n","                        else:\n","                            p.add_run(nm)\n","                        p.add_run(', ')\n","                    p.add_run(pap['text1'])\n","                    p.add_run(pap['text2'])\n","                    if pap['oudan']:\n","                        #print(pap['oudan'])\n","                        p.add_run(' ('+ pap['oudan']+')').font.color.rgb = RGBColor(0,0,255)\n","                        if pap['oudan'] in OudanDict.keys():\n","                            OudanDict[pap['oudan']]['count']+=1\n","                        else:\n","                            OudanDict[pap['oudan']]={}\n","                            OudanDict[pap['oudan']]['count']=1\n","                p=document.add_paragraph('...(計')\n","                p.add_run(str(inds)).font.color.rgb= RGBColor(255,0,0)\n","                p.add_run('件)')\n","                document.add_paragraph('')\n","                ws[papercelllist[i]]=inds\n","                i=i+1\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    # document.add_paragraph('共同研究リスト')\n","    # DKtext=['大学','企業・公共団体']\n","    # kokunaikaigaitext=['国内','海外']\n","\n","    # inputpositionsKyodo=['c'+'{0:02d}'.format(i) for i in range(32,44)]\n","    # outputpositionsKyodo=[col+'{0:02d}'.format(i) for col in ['G','I','K'] for i in range(47,51) ]\n","\n","    # for iK,oK in zip(inputpositionsKyodo,outputpositionsKyodo):\n","    #     ws[oK].value=name_dataform[iK].astype(float).sum()\n","\n","    # inputpositionsKyodo=['c'+'{0:02d}'.format(i) for i in range(44,56)]\n","    # outputpositionsKyodo=[col+'{0:02d}'.format(i) for col in ['H','J','L'] for i in range(47,51) ]\n","    # printtext=[yt + ' '+ kt +' '+ dk + ' '+ '契約書なし' for yt in yearstext  for dk in DKtext for kt in kokunaikaigaitext]\n","\n","    # for iK,oK,ptxt in zip(inputpositionsKyodo,outputpositionsKyodo,printtext):\n","    #     sumKyodo=0\n","    #     document.add_paragraph(ptxt)\n","    #     for a,piname in zip(name_dataform[iK],name_dataform['c02']):\n","    #         if (type(a)==str): \n","    #             if (len(a)>4): # なし、0などを省くために5文字以上\n","    #                 b=re.split('[、。,]', a)\n","    #                 sumKyodo+=len(b)\n","    #                 p =document.add_paragraph(piname+' 研: ')\n","    #                 for bb in b:\n","    #                     p.add_run(bb).underline = True\n","    #                     p.add_run('　')\n","\n","    #     # ws[oK].value=sumKyodo\n","    #     p=document.add_paragraph('...(計')\n","    #     p.add_run(str(sumKyodo)).font.color.rgb= RGBColor(255,0,0)\n","    #     p.add_run('件)')\n","    #     document.add_paragraph('')\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","\n","    document.add_paragraph('受賞リスト')\n","\n","    keys=list(AwardsDict.keys())\n","    datelist=[AwardsDict[r]['award_date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    inds=0\n","    for r in arg:\n","        if (AwardsDict[keys[r]]['award_date']>globalmindate) & (AwardsDict[keys[r]]['award_date']<globalmaxdate):\n","            inds=inds+1\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(AwardsDict[keys[r]]['name'])\n","            p.add_run(', '+AwardsDict[keys[r]]['award_name'])\n","            p.add_run(', '+AwardsDict[keys[r]]['association'])\n","            p.add_run(', '+AwardsDict[keys[r]]['award_date']+'.')\n","\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('国際会議招待講演リスト')\n","    keys=list(TalksDict.keys())\n","    datelist=[TalksDict[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","    inds=0\n","    for r in arg:\n","        if TalksDict[keys[r]][\"international\"] & (TalksDict[keys[r]]['date']>globalmindate) & (TalksDict[keys[r]]['date']<globalmaxdate) & (TalksDict[keys[r]][\"invited\"] | (TalksDict[keys[r]][\"keyoral\"] in ['invited_oral_presentation','keynote_oral_presentation','nominated_symposium'])):\n","            inds=inds+1\n","            pap=TalksDict[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            nm=pap[\"presenter\"]\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(', \\\"'+pap[\"presentation_title\"]+\"\\\"\")\n","            p.add_run(' ('+pap[\"keyoral\"]+')').font.color.rgb= RGBColor(255,0,0)\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(', '+pap[\"date\"]+'.')\n","\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('アウトリーチリスト')\n","    keys=list(SocialContDict.keys())\n","    datelist=[SocialContDict[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","\n","    inds=0\n","    for r in arg:\n","        pap=SocialContDict[keys[r]]\n","        if (SocialContDict[keys[r]]['date']>globalmindate) & (SocialContDict[keys[r]]['date']<globalmaxdate):\n","            inds=inds+1\n","            pap=SocialContDict[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(pap[\"name\"])\n","            p.add_run(', '+pap[\"title\"])\n","            p.add_run(', '+pap[\"event\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","            \n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('報道リスト')\n","    keys=list(medDict.keys())\n","    datelist=[medDict[r]['date'] for r in keys]\n","    arg=np.argsort(datelist)[::-1]\n","\n","    inds=0\n","    for r in arg:\n","        if (medDict[keys[r]]['date']>globalmindate) & (medDict[keys[r]]['date']<globalmaxdate):\n","            inds=inds+1\n","            pap=medDict[keys[r]]\n","            p = document.add_paragraph(str(inds)+'. ')\n","            p.add_run(pap[\"printname\"])\n","            p.add_run(','+pap[\"paper_title\"])\n","            p.add_run(pap[\"publication_name\"])\n","            p.add_run(' '+pap[\"date\"]+'.')\n","\n","    p=document.add_paragraph('')\n","    p.add_run().add_break(WD_BREAK.PAGE) # page break\n","\n","    document.add_paragraph('就職状況リスト')\n","\n","    for s in name_dataform['c22']:\n","        if type(s) is str:\n","            p = document.add_paragraph(s)\n","\n","    wb.save(file_name_xlsx)\n","    document.save(file_name_check)\n"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["if ('google.colab' in str(get_ipython())):\n","    files.download(file_name)\n","    if sankodata:\n","        files.download(file_name_xlsx)\n","        files.download(file_name_check)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"researchmapv2_to_docx.ipynb","provenance":[]},"interpreter":{"hash":"b6aaa8c452f1aa53401d42c477fe97baeea39d8952471e90cdbb23434ceb6c90"},"kernelspec":{"display_name":"Python 3.8.5 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"metadata":{"interpreter":{"hash":"b6aaa8c452f1aa53401d42c477fe97baeea39d8952471e90cdbb23434ceb6c90"}},"toc":{"colors":{"hover_highlight":"#DAA520","navigate_num":"#000000","navigate_text":"#333333","running_highlight":"#FF0000","selected_highlight":"#FFD700","sidebar_border":"#EEEEEE","wrapper_background":"#FFFFFF"},"moveMenuLeft":true,"nav_menu":{"height":"12px","width":"252px"},"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":4,"toc_cell":false,"toc_section_display":"block","toc_window_display":false,"widenNotebook":false}},"nbformat":4,"nbformat_minor":0}
